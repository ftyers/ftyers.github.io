<html>
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" >
  <link rel="stylesheet" type="text/css" href="../style.css" />
  <title>Class 3</title>

</head>
<body>
<div class="page">
<h2> Class 3 </h2>

<h3> Tracing program flow </h3>

<p>As your programs get more complicated, it will become increasingly more challenging to visualise what
they are doing mentally. A simple way to see what your program is doing is to use <tt>print</tt> 
statements which print to standard error, or <tt>stderr</tt>. </p>

<p>Take the program from the last class and add print statements as follows:</p>

<pre>
import sys

a = 0 # This is a variable for counting the number of vowels

for c in sys.stdin.read():
	print('Main loop:', a, c, file=sys.stderr)
	if c in 'аэиоуяеыёю':
		print('Found a vowel!', c, file=sys.stderr)
		a = a + 1

print(a)
</pre>

Now call the program (here called <tt>prog.py</tt>) from the command line like,

<pre>
$ head wiki.txt | python3 prog.py &gt; wiki.output
</pre>

<p>What do you see ? </p>

<p>Adding <tt>print</tt> statements like this for "debugging" is common and is also a good way to find out
what someone elses code is doing.</p>


<h3> Character encoding and special characters </h3>

On your computer, characters are stored as numbers, for example the word <em>cat</em>
is stored as <tt>0x63 0x61 0x74</tt> and the word <em>мир</em> is stored 
as <tt>0x43c 0x438 0x440</tt>. Try the following:

<pre>
>>> hex(ord(u'q'))
'0x71'
>>> hex(ord(u'я'))
'0x44f'
</pre>

Now open up your character map (for example <tt>gucharmap</tt>) and search for <tt>044f</tt>,
what do you find ? Now try:

<pre>
>>> hex(ord(u'е'))
'0x435'
>>> hex(ord(u'e'))
'0x65'
</pre>

What do you notice, what can you say about these two symbols ?
<p>
Why is all this important ? Well, there are a couple of instances when you need to know about
this stuff. One is when you are processing text from the web and there is the wrong 
encoding for some symbol (e.g. the character looks ok, but has the wrong underlying numerical
representation). This can have various reasons:</p>
<ul>
  <li> Misconfigured <span title="Оптическое распознавание символов"><u>optical-character recognition</u></span> (OCR) software that detects 
    Cyrillic characters as Latin characters </li>
  <li> Misconfigured keyboards that output the wrong characters </li>
</ul>
<p>Sometimes when you get some text and your program is not behaving properly, for example the tokenisation
messes up or your morphological analyser doesn't find the word but it is in the dictionary, it could
be because of this. </p>
<p>
It is also worth noting that this applies to spacing symbols in addition to characters. For 
example, there is a character <em>non-breaking space</em>. The normal spacing character 
is <tt>0x20</tt>, but non-breaking space is <tt>0xa0</tt>. Depending on the program or library
you use these may be treated as different characters or not.
</p>
<p>
If that wasn't enough, you may also come across the issue of pre-composed versus composed 
characters. For example if you see the character <em>ё</em> it could be encoded in two ways,
either with the single character <tt>0x451</tt> or with two characters <tt>0x435 0x308</tt>.
The character <tt>0x308</tt> is a <em>combining character</em> which means that it combines
with the previous character to make a new character.

<pre>
>>> hex(ord(u'ё'))
'0x451'
>>> hex(ord(u'ё'))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: ord() expected a character, but string of length 2 found
</pre>

You will see this when you try and look up the numerical value of the character, it throws
and error because the function expects a single character string, but you give it two characters (the 
letter <em>е</em> and the combining diaeresis).

Now try:

<pre>
>>> [hex(ord(c)) for c in u'ё']
['0x435', '0x308']
</pre>

When you process each character in the string separately, it works. The most important
thing to remember about character encoding when corpus processing is that what you see
might not have the same underlying representation to that which you expect.

<h3> Lists and tuples </h3>
<p>
You have already come across something that looks like a list in the second class. Like lists,
a string is a kind of sequence, that you can iterate over and address by index. Furthermore, 
when we split the string using <tt>split()</tt> the function returned a list of strings.
</p>
<p>
However, lists can include more than just strings. For example, lists can store fixed-length lists
of values that we call <em>tuples</em>. 
</p>

<pre>
>>> spisok = []
>>> spisok.append((1, 'а'))
>>> spisok.append((2, 'б'))
>>> spisok.append((3, 'в'))
>>> spisok
[(1, 'а'), (2, 'б'), (3, 'в')]
>>> spisok[1]
(2, 'б')
>>> spisok[1][0]
2
>>> spisok[1][1]
'б'
</pre>

Lists can also contain other lists, which are addressed in the same way as tuples:

<pre>
>>> spisok = []
>>> spisok.append(['а', 'б', 'в'])
>>> spisok.append(['a', 'b', 'v'])
>>> spisok
[['а', 'б', 'в'], ['a', 'b', 'v']]
</pre>

When you are embedding lists within other lists it can be easy to get confused about the indexing, so
note how <tt>append()</tt> works differently when applied to the outer list <tt>spisok.append('г')</tt> and 
the inner list <tt>spisok[0].append('г')</tt>.

<pre>
>>> spisok[0].append('г')
>>> spisok
[['а', 'б', 'в', 'г'], ['a', 'b', 'v']]
>>> spisok.append('г')
>>> spisok
[['а', 'б', 'в', 'г'], ['a', 'b', 'v'], 'г']
</pre>

<p>List slicing works the same way as string slicing. So you can also refer to the end of the list with <tt>-1</tt></p>

<pre>
>>> spisok
[['а', 'б', 'в'], ['a', 'b', 'v']]
>>> spisok[-1]
['a', 'b', 'v']
>>> spisok[-1][-1]
'v'
</pre>

<!-- stack 
<p>You can also use a list as a <span title="стек (стопка)"><u>stack</u></span>. A stack is a 
datastructure where you have some storage and two operations, <tt>push()</tt> and <tt>pop()</tt>.
When you <tt>push</tt> something onto the stack it goes on top of anything that is already there,
when you <tt>pop</tt> something, it takes the value from the top of the stack and removes it.</p>

<pre>
import sys

c = sys.stdin.read(1)
buffer = []
while c:
        if c == '\n':
                while buffer != []:
                        sys.stdout.write(buffer.pop())
                sys.stdout.write(c)
                c = sys.stdin.read(1)
                continue
        buffer.append(c)
        c = sys.stdin.read(1)
</pre>

Note: In Python, the <tt>push()</tt> operator is called <tt>append()</tt>

What does this code do ? How does it work ? Can you think of a Unix command that does something 
similar ? Can you think of a simpler way to implement the same thing in Python ? (tip: try looking 
on StackOverflow)
-->

<h3> Formatting output with <tt>print()</tt> </h3>



<pre>
>>> index = 1
>>> print('# sent_id = %d' % (index))
# sent_id = 1
</pre>

<pre>
>>> tekst = 'Привет мир!'
>>> print('# text = %s' % (tekst))
# text = Привет мир!
</pre>

If you try and print a string as a numeral you will get an error. But note that you won't get 
the same error if you try and print a numeral as a string.

<pre>
>>> print('# text = %s' % (index))
# text = 1
>>> print('# text = %d' % (tekst))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: %d format: a number is required, not str
</pre>

Numeral values are stored to a certain amount of precision in Python, but usually you don't
want to print them out to such a high precision. A rule of thumb is to think very hard before
giving more than four decimals. A better rule is to think about what you are measuring and
try and report as few decimal points as is informative.

<pre>
>>> p = 1/3
>>> p
0.3333333333333333
>>> print('p = %.2f' % (p))
p = 0.33
>>> print('p = %.4f' % (p))
p = 0.3333
</pre>

In Python, to report decimal points use the above syntax. The dot stands for the decimal point
then the number of decimals and finally the <tt>f</tt> stands for "<span title="число с плавающей запятой"><u>float</u></span>".

<h3> Tokenisation </h3>

<p>
So, let's say our sentence segmenter is outputting our sentences on one per line, what is the next step we need to do 
before we start some serious processing ?
</p>
<pre>
Это течение следует на север до Японского побережья, оказывая заметное влияние на климат японского побережья. 
Далее течение разделяется на два рукава: один отклоняется к югу и частью питает Экваториальное противотечение, а частью растекается по бассейнам индонезийских морей. 
У 40° с. ш. Куросио переходит в Северное Тихоокеанское течение, следующее на восток к побережью Орегона. 
</pre>
<p>
We need to split the text into tokens, that means treat each word as a word in its own right. This is more difficult
than it may appear. 
</p>
<p>
The objective is to take output like a block of text like the one above and produce output like the following:
</p>

<pre>
# sent_id = 1
# text = Это течение следует на север до Японского побережья, оказывая заметное влияние на климат японского побережья.
1	Это	_	_	_	_	_	_	_	_
2	течение	_	_	_	_	_	_	_	_
3	следует	_	_	_	_	_	_	_	_
4	на	_	_	_	_	_	_	_	_
5	север	_	_	_	_	_	_	_	_
6	до	_	_	_	_	_	_	_	_
7	Японского	_	_	_	_	_	_	_	_
8	побережья	_	_	_	_	_	_	_	_
9	,	_	_	_	_	_	_	_	_
10	оказывая	_	_	_	_	_	_	_	_
11	заметное	_	_	_	_	_	_	_	_
12	влияние	_	_	_	_	_	_	_	_
13	на	_	_	_	_	_	_	_	_
14	климат	_	_	_	_	_	_	_	_
15	японского	_	_	_	_	_	_	_	_
16	побережья	_	_	_	_	_	_	_	_
17	.	_	_	_	_	_	_	_	_

# sent_id = 2
# text = Далее течение разделяется на два рукава: один отклоняется к югу и частью питает Экваториальное противотечение, а частью растекается по бассейнам индонезийских морей.
1	Далее	_	_	_	_	_	_	_	_
2	течение	_	_	_	_	_	_	_	_
3	разделяется	_	_	_	_	_	_	_	_
4	на	_	_	_	_	_	_	_	_
5	два	_	_	_	_	_	_	_	_
6	рукава	_	_	_	_	_	_	_	_
7	:	_	_	_	_	_	_	_	_
8	один	_	_	_	_	_	_	_	_
9	отклоняется	_	_	_	_	_	_	_	_
10	к	_	_	_	_	_	_	_	_
11	югу	_	_	_	_	_	_	_	_
12	и	_	_	_	_	_	_	_	_
13	частью	_	_	_	_	_	_	_	_
14	питает	_	_	_	_	_	_	_	_
15	Экваториальное	_	_	_	_	_	_	_	_
16	противотечение	_	_	_	_	_	_	_	_
17	,	_	_	_	_	_	_	_	_
18	а	_	_	_	_	_	_	_	_
19	частью	_	_	_	_	_	_	_	_
20	растекается	_	_	_	_	_	_	_	_
21	по	_	_	_	_	_	_	_	_
22	бассейнам	_	_	_	_	_	_	_	_
23	индонезийских	_	_	_	_	_	_	_	_
24	морей	_	_	_	_	_	_	_	_
25	.	_	_	_	_	_	_	_	_

# sent_id = 3
# text = У 40° с. ш. Куросио переходит в Северное Тихоокеанское течение, следующее на восток к побережью Орегона.
1	У	_	_	_	_	_	_	_	_
2	40°	_	_	_	_	_	_	_	_
3	с. ш.	_	_	_	_	_	_	_	_
4	Куросио	_	_	_	_	_	_	_	_
5	переходит	_	_	_	_	_	_	_	_
6	в	_	_	_	_	_	_	_	_
7	Северное	_	_	_	_	_	_	_	_
8	Тихоокеанское	_	_	_	_	_	_	_	_
9	течение	_	_	_	_	_	_	_	_
10	,	_	_	_	_	_	_	_	_
11	следующее	_	_	_	_	_	_	_	_
12	на	_	_	_	_	_	_	_	_
13	восток	_	_	_	_	_	_	_	_
14	к	_	_	_	_	_	_	_	_
15	побережью	_	_	_	_	_	_	_	_
16	Орегона	_	_	_	_	_	_	_	_
17	.	_	_	_	_	_	_	_	_

</pre>
<p>
Here every token has been separated onto its own line and numbered. This kind of formatting for text is a subset of the 
popular <a href="http://universaldependencies.org/format.html">CoNLL-U</a> format used by the Universal Dependencies
project. There are ten <span title="столбцы"><u>columns</u></span> in total.
</p>
<div style="text-align:center">
<table>

<tr><td><b>Index</b></td><td><b>Surface form</b></td><td><b>Lemma</b></td><td><b>UPOS</b></td><td><b>XPOS</b></td><td><b>Morph. features</b></td><td><b>Head</b></td><td><b>Relation</b></td><td><b>Secondary dependencies</b></td><td><b>Miscellaneous</b></td></tr>
<tr><td><tt>1</tt></td><td><tt>У</tt></td><td><tt>_</tt></td><td><tt>_</tt></td><td><tt>_</tt></td><td><tt>_</tt></td><td><tt>_</tt></td><td><tt>_</tt></td><td><tt>_</tt></td><td><tt>_</tt></td></tr>
</table>
</div>
</pre>
<p>
A simple approach, and the one we are going to first, would simply be to replace every space ' ' with 
a newline character '\n'. Following that you can use the <tt>replace()</tt> function to preprocess the text
to add a space before or after certain punctuation characters like ',', ':', '(' and ')'. Can you write a program to do that? The program should be called <tt><b>tokeniser.py</b></tt> and you should upload it to your GitHub.
</p>
<p>
It should be possible to run your segmenter and tokeniser in a pipeline as follows:

<pre>
$ cat corpus/wiki.txt | python3 segmenter.py | python3 tokeniser.py
</pre>
</p>

<h3> Questions </h3>

<ul>
  <li> Why should we split punctuation from the token it goes with ? </li>
  <li> Should abbreviations with space in them be written as a single token or two tokens ?  </li>
  <ul>
    <li>How about numerals like 134&nbsp;000 ?</li>
  </ul> 
  <li> If you have a case suffix following punctuation, how should it be tokenised ?</li>
  <li> Should contractions and clitics be a single token or two (or more) tokens ?</li>
</ul>

</div>

</body>
</html>
