<html>
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" >
  <link rel="stylesheet" type="text/css" href="../style.css" />
  <title>Class 5</title>

</head>
<body>
<div class="page">

<h2> Class 5 </h2>


<h3>Regular expressions</h3>

You got a bit familiar with regular expressions in the <a href="01.html">tutorial</a> on the command line with <tt>sed</tt> and
<tt>grep</tt>. If you enjoyed that, you will be pleasantly surprised that it's also possible to use
regular expressions in Python! There is a nice tutorial on the Python site <a href="https://docs.python.org/3/library/re.html">here</a>,
but here are the highlights:

You can find a matching part of a string using <tt>search()</tt>:

<pre>
>>> import re 

>>> s = 'Привет мир!'

>>> re.search(r'м[а-я]+', s)
<_sre.SRE_Match object; span=(7, 10), match='мир'>

>>> m = re.search(r'м[а-я]+', s)
>>> m.group()
'мир'
>>> m.span()
(7, 10)
</pre>

The <tt>re.search()</tt> function takes a regular expression and a string returns a match object if it finds a matching part of the string. The match
object contains two important functions, <tt>group()</tt> gives you the part
of the string that matches the expression, while <tt>span()</tt> gives you 
the beginning and ending index of the match.

If you want to replace parts of a string, you can use <tt>sub()</tt> which works a bit like <tt>sed</tt>:

<pre>
>>> re.sub('[а-я]', 'x', s)
'Пxxxxx xxx!'
</pre>

<h3>Libraries and modules in Python</h3>

In this section I will briefly introduce three Python libraries/modules. A
library or module is a collection of code that you can <tt>import</tt> and
use in your own projects. Here are the three I will cover:

<ul>
  <li><tt>matplotlib</tt> for plotting graphs and data visualisation</li>
   <li><tt>ElementTree</tt> for parsing XML and well-formed HTML</li>
   <li><tt>scikit learn</tt> for doing machine learning</li>
</ul>

You should go through at least one, but it is not compulsory, although
it is recommended, to go through all three.

<h4><tt>matplotlib</tt></h4>

One really cool library in Python is <a href="https://matplotlib.org"><tt>matplotlib</tt></a>, this
is a library for visualising data and plotting graphs.

In your virtual machine you can install it as follows:

<pre>
$ sudo apt-get install python3-matplotlib
</pre>

Now you can try making a small program to take the frequency/rank list that you had earlier 
and plot it on a graph.

First make sure that you have the ranks file:

<pre>
$ head ranks.txt 
1	11324	.
2	11221	,
3	3972	æмæ
4	3121	:
5	2745	)
6	2732	(
7	2616	у
8	1708	азы
9	1646	йæ
10	1301	"
</pre>

Then try this program:

<pre>
import sys
import matplotlib.pyplot as plt

x = []
y = []

fd = open('ranks.txt', 'r')
for line in fd.readlines():
        line = line.strip()
        if line == '':
                continue
	
        row = line.split('\t')
        x.append(int(row[0]))
        y.append(int(row[1]))

plt.plot(x, y, 'ro')
plt.show()
</pre>

<!--<h4> <tt>numpy</tt> </h4> -->
<h4> ElementTree </h4> 

<a href="https://docs.python.org/3/library/xml.etree.elementtree.html">ElementTree</a> is a Python module for parsing XML. XML is a file format that is widely 
used for storing structured data. The <a href="http://depts.washington.edu/uwcl/odin/">ODIN
project</a> is a project which aims to collect and encode interlinear glossed text
in very many languages. You can find an example file for Icelandic
<a href="http://depts.washington.edu/uwcl/odin/isl-ex.xml">here</a>.

There is a lot of data there, but let's say we just wanted to pull out part of it, for 
example the normalised gloss. First download the file:

<pre>
$ wget "http://depts.washington.edu/uwcl/odin/isl-ex.xml"
</pre>

One way to get the information would be to just do string processing, but a better way
is to use the structure and an XML parser. The first thing to do is to import
ElementTree and use the <tt>parse()</tt> function to parse the XML.

<pre>
import xml.etree.ElementTree as ET

tree = ET.parse('isl-ex.xml')
</pre>

The first thing we can do is look at the tag of the root node in the tree:

<pre>
root = tree.getroot()

print(root.tag)
</pre>

We can also pull out nodes in the tree that have a specific attribute&mdash;value pair
by doing a search for the node type:
<!--<tier id="n" type="odin" alignment="c" state="normalized">-->

<pre>
for tier in root.findall('.//tier'):
        if tier.attrib['id'] == 'n':
                for item in tier.findall('.//item'):
                        print(item.text)
</pre>

If you save this into a file called <tt>gloss.py</tt>, you should get the output:

<pre>
$ python3 gloss.py 
(Þau) Jón og María eru vinir.
they.NEUT Jón og María are friends
Jón and María are friends.
</pre>

How would you get just the Icelandic line and the gloss line ? 

<h4> <tt>scikit learn</tt></h4>

There are many packages for <span title="машинное обучение"><u>machine learning</u></span> in Python, but one popular one is <tt>scikit learn</tt>. It contains a lot of different machine learning algorithms. 

You can install it by using the following command:
<pre>
$ sudo apt-get install python3-sklearn
</pre>

The example we're going to work through here is going to use the <span title="перцептрон"><u>perceptron</u></span>, one of the simplest (and in my opinion most beautiful) machine learning algorithms. The simple perceptron as described here is not used much in practice, as it
can only learn to classify problems that are <span title="линейно разделимый"><u>linearly separable</u></span>,
but networks of perceptrons (neural networks) are widely used. The perceptron is like a single neuron, it has one or more inputs, weights, 
an activation function and a single output.
 
The perceptron takes training examples with the following form:

<ul>
  <li>feature vector of binary (0, 1) numbers, and </li>
  <li>an output label</li>
</ul>

and builds up a weight vector where each weight corresponds to a cell in
the feature vector. 

For example, let's suppose we want to build a classifier to determine 
the pronunciation of <em>в</em> in Russian, the output label from the classifier
should be either /f/ or /v/. The feature vector might contain 
features such as "is the previous letter a vowel?", "is it at the beginning
of the word?", etc.

Let's suppose we had training data that looked like the following, where <tt>#</tt> stands for beginning or end of word:

<center>
<table>
<!--	# -1.isVowel -1.isUnvoiced -1.isVoiced -1.isBoundary 1.isVowel 1.isUnvoiced 1.isVoiced 1.isBoundary-->

  <tr><td><b>Word</b></td><td><b>IPA</b></td><td></td><td colspan="4"><b><span style="color:#3f704d">Previous character</span></b></td><td colspan="4"><b><span style="color:#014666">Next character</span></b></td><td><b>Label</b></td></tr>
  <tr><td></td>    <td></td>   <td></td>                <td>Vowel</td><td>Unvoiced</td><td>Voiced</td><td>#</td><td>Vowel</td><td>Unvoiced</td><td>Voiced</td><td>#</td></td>
  <tr><td>#пог<b><span style="color:#3f704d">о</span>в<span style="color:#014666">о</span></b>рка#</td><td>[pəɡɐˈvorkə]</td><td></td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>/v/</td></tr>
  <tr><td>#<b><span style="color:#3f704d">т</span>в<span style="color:#014666">о</span></b>рец#</td><td>[tvɐˈrʲet͡s]</td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>/v/</td></tr>
<tr><td>#резе<b><span style="color:#3f704d">р</span>в<span style="color:#014666">#</span></b></td><td>[rʲɪˈzʲerf]</td><td></td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>/f/</td></tr>
<tr><td>#ры<b><span style="color:#3f704d">т</span>в<span style="color:#014666">и</span></b>на#</td><td>[ˈrɨtvʲɪnə]</td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>/v/</td></tr>
<tr><td>#пьянс<b><span style="color:#3f704d">т</span>в<span style="color:#014666">о</span></b>#</td><td>[ˈpʲjanstvə]</td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>/v/</td></tr>
<tr><td>#розл<b><span style="color:#3f704d">и</span>в<span style="color:#014666">#</span></b></td><td>[ˈroz⁽ʲ⁾lʲɪf]</td><td></td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>/f/</td></tr>
<tr><td>#<b><span style="color:#3f704d">с</span>в<span style="color:#014666">а</span></b>рка#</td><td>[ˈsvarkə]</td><td></td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>/v/</td></tr>
<tr><td>#<b><span style="color:#3f704d">я</span>в<span style="color:#014666">к</span></b>а#</td><td>[ˈjafkə]</td><td></td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>/f/</td></tr>
<tr><td></td><td></td><td></td><td></td><td>

</table>
</center>

We could generate this kind of data fairly easily from the pages on Wiktionary (see the section on "Screenscraping" below).

<pre>
0	#поговорка#	[pəɡɐˈvorkə]	1,0,0,0,1,0,0,0
0	#творец#	[tvɐˈrʲet͡s]	0,1,0,0,1,0,0,0
1	#резерв#	[rʲɪˈzʲerf]	0,0,0,0,0,0,0,1
0	#рытвина#	[ˈrɨtvʲɪnə]	0,1,0,0,1,0,0,0
0	#пьянство#	[ˈpʲjanstvə]	0,1,0,0,1,0,0,0
1	#розлив#	[ˈroz⁽ʲ⁾lʲɪf]	1,0,0,0,0,0,0,1
0	#сварка#	[ˈsvarkə]	0,1,0,0,1,0,0,0
1	#явка#	[ˈjafkə]	1,0,0,0,0,1,0,0
</pre>

There is a full set of training data (based on the Russian words with <em>в</em> in them in the SynTagRus corpus) 
available for download <a href="http://xixona.dlsi.ua.es/~fran/pronunciation_data.tsv">here</a>. Download
it using <tt>wget</tt>:

<pre>
$ wget "http://xixona.dlsi.ua.es/~fran/pronunciation_data.tsv"
</pre>

Now make a new file called <tt>pronunciation.py</tt>:

<pre>
import sys
from sklearn.linear_model import perceptron

words = []    # The word, correct label and pronunciation
data = []     # Training examples, e.g. feature vectors
labels = []   # Correct labels
</pre>

Here we import the classifier from the <tt>scikit learn</tt> package and set up some variables.

<pre>
for line in open('pronunciation_data.tsv').readlines():
	row = line.strip().split('\t')
        vec = []
        for i in row[3].split(','):
                vec.append(int(i))
        data.append(vec)
	labels.append(int(row[0]))
	words.append((row[1], row[2], int(row[0])))
</pre>

This part parses the <tt>pronunciation_data.tsv</tt> file and puts the data into a form 
that the classifier can understand.

<pre>
net = perceptron.Perceptron(n_iter=100, verbose=0, random_state=None, fit_intercept=True, eta0=0.002)
net.fit(data,labels)
</pre>

To train the classifier we just need to create a new <tt>Perceptron</tt> object and then run
the training procedure <tt>fit()</tt>

<pre>
result = net.predict(data)
</pre>

We can then run the classifier on the training data to see how accurate it is, and print out the 
accuracy:

<pre>
total = 0
correct = 0
for i in range(0, len(words)):
	if result[i] == words[i][2]:
		print('+', result[i], words[i]);
		correct = correct + 1
	else:
		print('-', result[i], words[i]);
	total = total + 1
print(correct/total)
</pre>

<pre>
$ python3 pronunciation.py
</pre>

An exercise for the reader is to split the data in two randomly and train on one half and test
on the other half. What kind of accuracy do you get ? What kind of 
errors does the classifier make ? How do you think you might be able to improve on the accuracy ?

<!--<h4> </h4> -->

<h3>Screenscraping</h3>

Screenscraping (or web scraping) is the task of pulling human-readable data from the web and turning it 
into machine-readable form. There are two main components, the first is <em>crawling</em> which means
downloading the pages you are interested in. The second is <em>scraping</em> which means getting the 
information out of those pages. We're going to flip things around and start with the second first.

Run this command:

<pre>
$ wget -q -O - "http://ru.wiktionary.org/wiki/страх" 
</pre>

The output is pretty terrifying right ? A tonne of HTML code, very difficult to find the structure or data. But if 
we go to the page <a href="http://ru.wiktionary.org/wiki/страх">страх</a> there is a lot of useful data. What kind
of data might we be interested in ? 

<ul>
<li> Stem </li>
<li> Inflection table </li>
<li> Zaliznjak code </li>
<li> Definitions </li>
<li> Translations </li>
<li> Pronunciation </li>
<li> ... </li>
</ul>

Let's start with a simple exercise to extract the stem, the pronunciation and the Zaliznjak code. Download
the page and save it to a file called <tt>страх.html</tt> with <tt>wget</tt> or <tt>curl</tt>. Now look
through the HTML and find the lines where the information you are looking for is.

Here is the line with the stem,

<pre>
&lt;p&gt;Корень: &lt;b&gt;-страх-&lt;/b&gt;.&lt;/p&gt;
</pre>

Here is the line with the pronunciation,

<pre>
&lt;li&gt;&lt;a href="https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D0%B6%D0%B4%D1%83%D0%BD%D0%B0%D1%80%D0%BE%D0%B4%D0%BD%D1%8B%D0%B9_%D1%84%D0%BE%D0%BD%D0%B5%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B0%D0%BB%D1%84%D0%B0%D0%B2%D0%B8%D1%82" class="extiw" title="w:Международный фонетический алфавит"&gt;МФА&lt;/a&gt;: ед.&amp;#160;ч.&amp;#160;[&lt;span class="IPA" style="white-space: nowrap;"&gt;strax&lt;/span&gt;]
</pre>

And here is the line with the Zaliznjak code,

<pre>
&lt;p&gt;&lt;a href="/wiki/%D1%81%D1%83%D1%89%D0%B5%D1%81%D1%82%D0%B2%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%BE%D0%B5" title="существительное"&gt;Существительное&lt;/a&gt;, неодушевлённое, мужской род, 2-е &lt;a href="/wiki/%D1%81%D0%BA%D0%BB%D0%BE%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5" title="склонение"&gt;склонение&lt;/a&gt; (тип склонения 3a по &lt;a href="/wiki/%D0%92%D0%B8%D0%BA%D0%B8%D1%81%D0%BB%D0%BE%D0%B2%D0%B0%D1%80%D1%8C:%D0%98%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D1%81%D0%BB%D0%BE%D0%B2%D0%B0%D1%80%D1%8F_%D0%97%D0%B0%D0%BB%D0%B8%D0%B7%D0%BD%D1%8F%D0%BA%D0%B0" title="Викисловарь:Использование словаря Зализняка"&gt;классификации А.&amp;#160;А.&amp;#160;Зализняка&lt;/a&gt;).&lt;/p&gt;
</pre>

We might decide that all of this would be a lot easier to process if we didn't have the HTML code in the 
way... that might not always be the case. But we could try a simple function like the following 
to get rid of (most of) the HTML:

<pre>
def strip_html(h):
	o = ''
	inTag = False
	for c in h: 
		if c == '<':
			inTag = True
			continue
		if c == '>':
			inTag = False
			continue
		if not inTag:
			o = o + c
	return o	
</pre>

Now let's see what we get:

<pre>
Корень: -страх-.
</pre>

<pre>
МФА: ед.&amp;#160;ч.&amp;#160;[strax]
</pre>

<pre>
Существительное, неодушевлённое, мужской род, 2-е склонение (тип склонения 3a по классификации А.&amp;#160;А.&amp;#160;Зализняка).
</pre>

Ok, this is clearer, let's come up with some heuristics:

<ul>
  <li> The stem is found on the line that starts with 'Корень' and appears after the ':' and
before the '.'</li>
  <li> The pronunciation is found on the line that contains 'МФА' and comes between '[' and ']'. </li>
  <li> The declension type is on the line that contains 'тип склонения' and appears immediately after that.</li>
</ul>

If we read through the file line-by-line we can use these heuristics to identify where the information
is that we want to extract and how to extract it.

Take a look at the following code:

<pre>
tem = '_'
zkod = '_'
ipa = '_'
for line in sys.stdin.readlines(): 
        line = line.strip()
        text = strip_html(line);

        if text.count('Корень:') > 0:
                stem = text.split(':')[1].strip('.')
        if text.count('МФА') > 0:
                ipa = text.split('[')[1].split(']')[0]
        if text.count('тип склонения') > 0:
                zkod = text.split('тип склонения')[1].strip().split(' ')[0]

        if stem != '_' and zkod != '_' and ipa != '_':
                print('%s\t%s\t%s' % (stem, zkod, ipa))
                stem = '_'
                zkod = '_'
                ipa = '_'
</pre>

And try running it on your <tt>страх.html</tt> file. You should get output something like:

<pre>
$ cat страх.html | python3 wiktionary.py
 -страх-	3a	strax
</pre>

Great! But there is a chance our code is a bit fragile, so we should try it with a 
few more pages. Let's try it with the page <a href="https://ru.wiktionary.org/wiki/%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE">дерево</a>.

<pre>
$ cat дерево.html | python3 wiktionary.py 
 -дерев-; окончание	1a^	ˈdʲerʲɪvə
 --	по	ˈd̪ɛ.rɛ̝.wɔ
</pre>

Uh oh, this isn't exactly what we were looking for. There are two main issues:

<ul>
  <li> It also picks up the information from the Ukrainian section of the page. </li>
  <li> In this particular page there is more information regarding the stem, you also get the ending.</li>
</ul>

So how can we fix our code... Let's look at the HTML again. 

<pre>
&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h1&gt;&lt;span class="mw-headline" id=".D0.A0.D1.83.D1.81.D1.81.D0.BA.D0.B8.D0.B9"&gt;Русский&lt;/span&gt;&lt;/h1&gt;
&lt;h3&gt;&lt;span class="mw-headline" id=".D0.9C.D0.BE.D1.80.D1.84.D0.BE.D0.BB.D0.BE.D0.B3.D0.B8.D1.87.D0.B5.D1.81.D0.BA.D0.B8.D0.B5_.D0.B8_.D1.81.D0.B8.D0.BD.D1.82.D0.B0.D0.BA.D1.81.D0.B8.D1.87.D0.B5.D1.81.D0.BA.D0.B8.D0.B5_.D1.81.D0.B2.D0.BE.D0.B9.D1.81.D1.82.D0.B2.D0.B0"&gt;Морфологические и синтаксические свойства&lt;/span&gt;&lt;/h3&gt;
</pre>

So, the sections on the page are implemented with the <tt>&lt;h1&gt;</tt> tag in HTML. We can use a 
variable to track what heading we're in and skip the section if it isn't Russian. For example:

<pre>
h1 = '_';
for line in sys.stdin.readlines(): 
	if line.count('&lt;h1&gt;') > 0: 
		h1 = strip_html(line)

	if h1 != 'Русский': 
		continue

	...
</pre>

I leave the second problem (dealing with the <em>окончание</em>) as an exercise for the reader.

<h3>Unigram tagger</h3>

<p>
A unigram tagger works by assiging the most frequent tag seen for a given
surface form. It may additionally <em>back-off</em> to the most frequent
tag in general. Our unigram language model has all of this information, we 
just need to apply it to our input stream.
</p>

<p>
The first thing your program will need to do is load the language model and 
calculate the most frequent tag per word and most frequent tag in general. You
can use a <tt>dict</tt> for the word → tag mapping and a simple variable
for the most frequent tag.
</p>

Example input:

<pre>
# sent_id = 482
# text = Часть его положений отменена Конституционным судом в 2013 году.
1	Часть	_	_	_	_	_	_	_	_
2	его	_	_	_	_	_	_	_	_
3	положений	_	_	_	_	_	_	_	_
4	отменена	_	_	_	_	_	_	_	_
5	Конституционным	_	_	_	_	_	_	_	_
6	судом	_	_	_	_	_	_	_	_
7	в	_	_	_	_	_	_	_	_
8	2013	_	_	_	_	_	_	_	_
9	году	_	_	_	_	_	_	_	_
10	.	_	_	_	_	_	_	_	_

</pre>

Output:

<pre>
# sent_id = 482
# text = Часть его положений отменена Конституционным судом в 2013 году.
1	Часть	_	NOUN	_	_	_	_	_	_
2	его	_	NOUN	_	_	_	_	_	_
3	положений	_	NOUN	_	_	_	_	_	_
4	отменена	_	NOUN	_	_	_	_	_	_
5	Конституционным	_	NOUN	_	_	_	_	_	_
6	судом	_	NOUN	_	_	_	_	_	_
7	в	_	ADP	_	_	_	_	_	_
8	2013	_	NOUN	_	_	_	_	_	_
9	году	_	NOUN	_	_	_	_	_	_
10	.	_	PUNCT	_	_	_	_	_	_
</pre> <!-- 70% correct -->

The program should be called <tt><b>tagger.py</b></tt>. You should upload it to your GitHub and it should 
be able to be run in the pipeline with your other programs, for example:
<pre>
$ cat corpus/wiki.txt | python3 segmenter.py | python3 tokeniser.py | python3 tagger.py model.tsv
</pre>
Where <tt>model.tsv</tt> is the language model generated by your unigram language model training 
script (see <a href="https://ftyers.github.io/079-osnov-programm/classes/04_2.html">Class 4.2</a>).

<h3> Questions </h3>

<ul>
  <li> How accurate is the tagger ?</li>
  <li> How could you improve performance without incorporating context ...</li>
  <ul>
    <li> using Python string functions ? </li>
    <li> using regular expressions ? </li>
    <li> using screen scraping ? </li>
  </ul>
  <li> Could you store other single-word features in your unigram model ? Which features might you like to store ?</li>
</ul>

<h3> Further reading </h3>

<ul>
  <li> <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup</a> is a nice Python
    library for pulling data out of HTML. It is a more advanced technique than the one I have described here.</li>
</ul>
</div>

</body>
</html>


