{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Downloaded train and test set of UD_Japanese-GSD\n",
    "\n",
    "2) Got forms and sentences files for both sets using recommended commands in bash\n",
    "\n",
    "\n",
    "cat ja_gsd-ud-train.conllu  | grep '^[0-9]' | cut -f2 | sort -f | uniq -c | sort -gr > forms.txt\n",
    "\n",
    "\n",
    "$ cat ~/source/UniversalDependencies/UD_Japanese-GSD/ja_gsd-ud-train.conllu | grep '# text = ' | cut -f2 -d'=' | sed 's/^ *//g'> sentences.txt. \n",
    "\n",
    "Located files to my repo file\n",
    "\n",
    "3) Created dictionatry of the forms and check all existing lengths of extracted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary_length =  22313 total_lines =  161900\n"
     ]
    }
   ],
   "source": [
    "forms = \"C://mcl2018_hw1_babakov//practicals//Tokenisation//forms.txt\"\n",
    "dictionary = {}\n",
    "lines = 0\n",
    "with open(forms,'r',encoding = 'utf-8') as f: \n",
    "    for line in f:\n",
    "        lines += 1\n",
    "        if (line not in dictionary):\n",
    "            dictionary[line[:-1]] = 1\n",
    "    print('dictionary_length = ', len(dictionary), 'total_lines = ', lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info 4\n",
      "425001-435001-435501-425501 27\n",
      "22380 5\n",
      "20257/8 7\n",
      "2008-2009 9\n",
      "1447-1 6\n",
      "972 3\n",
      "673-0405 8\n",
      "99 2\n",
      "9 1\n",
      "ユーゴスラビア連邦軍 10\n",
      "福岡ソフトバンクホークス 12\n",
      "マイクロソフト株式会社 11\n",
      "voodoowedding 13\n",
      "VLCMediaPlayer 14\n",
      "UnitedMusicPublishersLtd 24\n",
      "TheMagazineofFantasy 20\n",
      "TheEmotionMachine 17\n",
      "TheCorporationoftheCityofSarnia 31\n",
      "theCHEMISTRYjointalbum 22\n",
      "TechnicalDesignReport 21\n",
      "TARGETfrontierJV 16\n",
      "TakamiyClassics 15\n",
      "PaulKantner'sWoodenship 23\n",
      "MozillaThunderbird 18\n",
      "http://en.wikipedia.org/wiki/Acute_intermittent_porphyria 57\n",
      "EnterpriseJavaBeans 19\n",
      "EconomicCommunitiyofWestAfricanStates 37\n",
      "Daisenguchistationmonument 26\n",
      "アニメポケットモンスターオリジナルサウンドトラックベスト 28\n"
     ]
    }
   ],
   "source": [
    "collected_length = []\n",
    "for name in dictionary:\n",
    "    length = len(name)\n",
    "    if (length not in collected_length):\n",
    "        print (name,length)\n",
    "        collected_length.append(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Implement MaxMatch using recursion  and apply it to train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============\n",
      " ホッケーにはデンジャラスプレーの反則があるので、膝より上にボールを浮かすことは基本的に反則になるが、その例外の一つがこのスクープである。\n",
      "\n",
      "['ホッケー', 'に', 'は', 'デンジャラスプレー', 'の', '反則', 'が', 'ある', 'ので', '、', '膝', 'より', '上', 'に', 'ボール', 'を', '浮かす', 'こと', 'は', '基本的', 'に', '反則', 'に', 'なる', 'が、', 'その', '例外', 'の', '一', 'つ', 'が', 'この', 'スクープ', 'である', '。'] \n",
      "\n",
      "============\n",
      " また行きたい、そんな気持ちにさせてくれるお店です。\n",
      "\n",
      "['また', '行き', 'たい', '、', 'そんな', '気持ち', 'に', 'させ', 'て', 'くれる', 'お', '店', 'です', '。'] \n",
      "\n",
      "============\n",
      " 手に持った特殊な刃物を使ったアクロバティックな体術や、揚羽と薄羽同様にクナイや忍具を使って攻撃してくる。\n",
      "\n",
      "['手', 'に', '持っ', 'た', '特殊', 'な', '刃物', 'を', '使っ', 'た', 'アクロバティック', 'な', '体', '術', 'や', '、', '揚羽', 'と', '薄羽', '同様', 'に', 'クナイ', 'や', '忍具', 'を', '使っ', 'て', '攻撃', 'し', 'て', 'くる', '。'] \n",
      "\n",
      "============\n",
      " 3年次にはトータルオフェンスで2,892ヤードを獲得し、これは大学記録となった。\n",
      "\n",
      "['3', '年次', 'に', 'は', 'トータルオフェンス', 'で', '2,892', 'ヤード', 'を', '獲得', 'し', '、', 'これ', 'は', '大学', '記録', 'と', 'なっ', 'た', '。'] \n",
      "\n",
      "============\n",
      " 葬儀の最中ですよ!\n",
      "\n",
      "['葬儀', 'の', '最中', 'です', 'よ', '!'] \n",
      "\n",
      "============\n",
      " 1998年度に着手し、道の駅遠山郷北側からかぐら大橋南詰現道交点までの1.060 kmのみ開通済み。\n",
      "\n",
      "['1998', '年度', 'に', '着手', 'し', '、', '道', 'の', '駅', '遠山郷', '北', '側', 'から', 'かぐら', '大橋', '南', '詰', '現道', '交点', 'まで', 'の', '1.060', 'km', 'のみ', '開通', '済み', '。'] \n",
      "\n",
      "============\n",
      " そして、第40話でそのカオルと百子をも失い、完全に孤児になってゲンとともに美山家に身を寄せ、またゲンに連れ添って円盤生物を調査するパートナーとなる。\n",
      "\n",
      "['そして', '、', '第', '40', '話', 'で', 'その', 'カオル', 'と', '百子', 'を', 'も', '失い', '、', '完全', 'に', '孤児', 'に', 'なっ', 'て', 'ゲン', 'とともに', '美', '山家', 'に', '身', 'を', '寄せ', '、', 'また', 'ゲン', 'に', '連れ添っ', 'て', '円盤', '生物', 'を', '調査', 'する', 'パートナー', 'と', 'なる', '。'] \n",
      "\n",
      "============\n",
      " 一般的な層流翼型と比べ負圧中心が前進し、圧力勾配はなだらかである。\n",
      "\n",
      "['一般的', 'な', '層流', '翼', '型', 'と', '比べ', '負', '圧', '中心', 'が', '前進', 'し', '、', '圧力', '勾配', 'は', 'なだらか', 'である', '。'] \n",
      "\n",
      "============\n",
      " 一直線に伸びる電撃を放ち、電撃ダメージを与える。\n",
      "\n",
      "['一', '直線', 'に', '伸びる', '電撃', 'を', '放ち', '、', '電撃', 'ダメージ', 'を', '与える', '。'] \n",
      "\n",
      "============\n",
      " 配属されて最初の1カ月こそ「親切な先輩に恵まれてラッキーだな」と思ったものの、その後すぐに「ちょっとおせっかいかな」と感じるようになり、半年後には「少し邪魔かも」と思い、1年後には「生産性の阻害要因だ」と確信するに至りました。\n",
      "\n",
      "['配属', 'さ', 'れ', 'て', '最初', 'の', '1', 'カ月', 'こそ', '「', '親切', 'な', '先輩', 'に', '恵まれ', 'て', 'ラッキー', 'だ', 'な', '」', 'と', '思っ', 'たも', 'の', 'の', '、', 'その後', 'すぐ', 'に', '「', 'ちょっと', 'おせっかい', 'かな', '」', 'と', '感じる', 'ように', 'なり', '、', '半年', '後', 'に', 'は', '「', '少し', '邪魔', 'か', 'も', '」', 'と', '思い', '、', '1', '年', '後', 'に', 'は', '「', '生産性', 'の', '阻害', '要因', 'だ', '」', 'と', '確信', 'する', 'に', '至り', 'まし', 'た', '。'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def MaxMatch(line):\n",
    "    parsed_line = []\n",
    "    parsed_rest_line = []\n",
    "    line = line.replace(\" \", \"\")\n",
    "    found_dict_symb = False\n",
    "    \n",
    "    for char_index in range(len(line)-1,0,-1):\n",
    "        phrase_under_review = ''\n",
    "        phrase_under_review = line[:char_index]\n",
    "        if (phrase_under_review in dictionary):\n",
    "            parsed_line.append(phrase_under_review)\n",
    "            if(len(line[char_index:]) >= 2):\n",
    "                parsed_rest_line = MaxMatch(line[char_index:])\n",
    "                parsed_line += parsed_rest_line\n",
    "                found_dict_symb = True\n",
    "            else:#if the next is empty\n",
    "                return parsed_line\n",
    "            \n",
    "            return parsed_line \n",
    "    if (found_dict_symb == False):\n",
    "        parsed_line += line[0]\n",
    "        if(len(line) >= 2):\n",
    "            parsed_rest_line = MaxMatch(line[1:])\n",
    "        else:\n",
    "            return parsed_line \n",
    "        \n",
    "        parsed_line += parsed_rest_line\n",
    "        return parsed_line \n",
    "\n",
    "text = \"C://mcl2018_hw1_babakov//practicals//Tokenisation//sentences.txt\"\n",
    "parsed_sentences = []\n",
    "with open(text,'r',encoding = 'utf-8') as t:\n",
    "    print_count = 0 \n",
    "    for line in t:\n",
    "        parsed = MaxMatch(line)\n",
    "        if(print_count < 10):\n",
    "            print(\"============\")\n",
    "            print(line)\n",
    "            print(parsed,'\\n')\n",
    "        parsed_sentences.append(parsed)\n",
    "        print_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Apply to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============\n",
      "﻿これに不快感を示す住民はいましたが,現在,表立って反対や抗議の声を挙げている住民はいないようです。\n",
      "\n",
      "['\\ufeff', 'これ', 'に', '不快', '感', 'を', '示す', '住民', 'は', 'いま', 'し', 'たが', ',', '現在', ',', '表', '立っ', 'て', '反対', 'や', '抗議', 'の', '声', 'を', '挙げて', 'いる', '住民', 'は', 'い', 'ない', 'ようで', 'す', '。'] \n",
      "\n",
      "============\n",
      "幸福の科学側からは,特にどうしてほしいという要望はいただいていません。\n",
      "\n",
      "['幸福', 'の', '科学', '側', 'から', 'は', ',', '特に', 'どうして', 'ほしい', 'という', '要望', 'は', 'いただい', 'て', 'いま', 'せ', 'ん', '。'] \n",
      "\n",
      "============\n",
      "星取り参加は当然とされ,不参加は白眼視される。\n",
      "\n",
      "['星', '取り', '参加', 'は', '当然', 'と', 'さ', 'れ', ',', '不', '参加', 'は', '白', '眼', '視', 'さ', 'れる', '。'] \n",
      "\n",
      "============\n",
      "室長の対応には終始誠実さが感じられた。\n",
      "\n",
      "['室長', 'の', '対応', 'に', 'は', '終始', '誠実', 'さ', 'が', '感じ', 'られ', 'た', '。'] \n",
      "\n",
      "============\n",
      "多くの女性が生理のことで悩んでいます。\n",
      "\n",
      "['多く', 'の', '女性', 'が', '生', '理', 'の', 'こと', 'で', '悩ん', 'で', 'いま', 'す', '。'] \n",
      "\n",
      "============\n",
      "先生の理想は限りなく高い。\n",
      "\n",
      "['先生', 'の', '理想', 'は', '限り', 'なく', '高い', '。'] \n",
      "\n",
      "============\n",
      "それは兎も角,私も明日の社説を楽しみにしております。\n",
      "\n",
      "['それ', 'は', '兎', 'も', '角', ',', '私', 'も', '明日', 'の', '社', '説', 'を', '楽しみ', 'に', 'し', 'て', 'おり', 'ます', '。'] \n",
      "\n",
      "============\n",
      "そうだったらいいなあとは思いますが,日本学術会議の会長談話について“当会では,標記の件について,以下のように考えます。”\n",
      "\n",
      "['そうだっ', 'たら', 'いい', 'なあ', 'と', 'は', '思い', 'ます', 'が', ',', '日本学術会議', 'の', '会長', '談', '話', 'について', '“', '当', '会', 'では', ',', '標記', 'の', '件', 'について', ',', '以下', 'のよ', 'う', 'に', '考え', 'ます', '。', '”'] \n",
      "\n",
      "============\n",
      "教団にとっては存続が厳しくなると思う。\n",
      "\n",
      "['教団', 'にとって', 'は', '存続', 'が', '厳しく', 'なる', 'と', '思う', '。'] \n",
      "\n",
      "============\n",
      "しかし強制していなくても問題です\n",
      "\n",
      "['しかし', '強制', 'し', 'て', 'い', 'なく', 'て', 'も', '問題', 'です'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"C://mcl2018_hw1_babakov//practicals//Tokenisation//sentence_test.txt\"\n",
    "parsed_sentences_test = []\n",
    "with open(text,'r',encoding = 'utf-8') as t:\n",
    "    print_count = 0 \n",
    "    for line in t:\n",
    "        parsed = MaxMatch(line)\n",
    "        if(print_count < 10):\n",
    "            print(\"============\")\n",
    "            print(line)\n",
    "            print(parsed,'\\n')\n",
    "        parsed_sentences_test.append(parsed)\n",
    "        print_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Write results to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"C://mcl2018_hw1_babakov//practicals//Tokenisation//sentence_test_parsed.txt\", 'w',encoding = 'utf-8') as f:\n",
    "    for t in parsed_sentences_test:\n",
    "        f.write(' '.join(str(s) for s in t) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Extract correctly tokenized test text for further evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found text  これに不快感を示す住民はいましたが,現在,表立って反対や抗議の声を挙げている住民はいないようです。\n",
      "['これ', 'に', '不快感', 'を', '示す', '住民', 'は', 'い', 'まし', 'た', 'が', ',', '現在', ',', '表立っ', 'て', '反対', 'や', '抗議', 'の', '声', 'を', '挙げ', 'て', 'いる', '住民', 'は', 'い', 'ない', 'よう', 'です', '。']\n",
      "found text  幸福の科学側からは,特にどうしてほしいという要望はいただいていません。\n",
      "['幸福', 'の', '科学', '側', 'から', 'は', ',', '特に', 'どうして', 'ほしい', 'という', '要望', 'は', 'いただい', 'て', 'い', 'ませ', 'ん', '。']\n",
      "found text  星取り参加は当然とされ,不参加は白眼視される。\n",
      "['星取り', '参加', 'は', '当然', 'と', 'さ', 'れ', ',', '不', '参加', 'は', '白眼視', 'さ', 'れる', '。']\n",
      "found text  室長の対応には終始誠実さが感じられた。\n",
      "['室長', 'の', '対応', 'に', 'は', '終始', '誠実', 'さ', 'が', '感じ', 'られ', 'た', '。']\n",
      "found text  多くの女性が生理のことで悩んでいます。\n",
      "['多く', 'の', '女性', 'が', '生理', 'の', 'こと', 'で', '悩ん', 'で', 'い', 'ます', '。']\n",
      "found text  先生の理想は限りなく高い。\n",
      "['先生', 'の', '理想', 'は', '限りなく', '高い', '。']\n",
      "found text  それは兎も角,私も明日の社説を楽しみにしております。\n",
      "['それ', 'は', '兎', 'も', '角', ',', '私', 'も', '明日', 'の', '社説', 'を', '楽しみ', 'に', 'し', 'て', 'おり', 'ます', '。']\n",
      "found text  そうだったらいいなあとは思いますが,日本学術会議の会長談話について“当会では,標記の件について,以下のように考えます。”\n",
      "['そう', 'だっ', 'たら', 'いい', 'なあ', 'と', 'は', '思い', 'ます', 'が', ',', '日本学術会議', 'の', '会長', '談話', 'について', '“', '当', '会', 'で', 'は', ',', '標記', 'の', '件', 'について', ',', '以下', 'の', 'ように', '考え', 'ます', '。', '”']\n",
      "found text  教団にとっては存続が厳しくなると思う。\n",
      "['教団', 'にとって', 'は', '存続', 'が', '厳しく', 'なる', 'と', '思う', '。']\n",
      "found text  しかし強制していなくても問題です\n",
      "['しかし', '強制', 'し', 'て', 'い', 'なく', 'て', 'も', '問題', 'です']\n"
     ]
    }
   ],
   "source": [
    "test_conllu = \"C://mcl2018_hw1_babakov//practicals//Tokenisation//ja_gsd-ud-test.conllu\"\n",
    "parsed_file = []\n",
    "print_count = 0\n",
    "with open(test_conllu, \"r\", encoding = 'utf-8') as data:\n",
    "    parsed_sentence = [] \n",
    "    for line in data:\n",
    "        row = line.split('\\t')\n",
    "        if('# text =' in row[0] and print_count < 10):\n",
    "            print('found text', row[0][8:-1])\n",
    "        if(str(row[0]).isdigit()):\n",
    "            parsed_sentence.append(row[1])\n",
    "        else:\n",
    "            if parsed_sentence:\n",
    "                parsed_file.append(parsed_sentence)\n",
    "                if (print_count < 10):\n",
    "                    print(parsed_sentence)\n",
    "                    print_count += 1\n",
    "            parsed_sentence = []\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Write results to sentences_test_correctly_parsed.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"C://mcl2018_hw1_babakov//practicals//Tokenisation//sentences_test_correctly_parsed.txt\", 'w',encoding = 'utf-8') as f:\n",
    "    for t in parsed_file:\n",
    "        f.write(' '.join(str(s) for s in t) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Evaluate maxmatch results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "def editDistance(r, h):\n",
    "\n",
    "    '''\n",
    "\n",
    "    This function is to calculate the edit distance of reference sentence and the hypothesis sentence.\n",
    "\n",
    "    Main algorithm used is dynamic programming.\n",
    "\n",
    "    Attributes: \n",
    "\n",
    "        r -> the list of words produced by splitting reference sentence.\n",
    "\n",
    "        h -> the list of words produced by splitting hypothesis sentence.\n",
    "\n",
    "    '''\n",
    "\n",
    "    d = numpy.zeros((len(r)+1)*(len(h)+1), dtype=numpy.uint8).reshape((len(r)+1, len(h)+1))\n",
    "\n",
    "    for i in range(len(r)+1):\n",
    "\n",
    "        for j in range(len(h)+1):\n",
    "\n",
    "            if i == 0: \n",
    "\n",
    "                d[0][j] = j\n",
    "\n",
    "            elif j == 0: \n",
    "\n",
    "                d[i][0] = i\n",
    "\n",
    "    for i in range(1, len(r)+1):\n",
    "\n",
    "        for j in range(1, len(h)+1):\n",
    "\n",
    "            if r[i-1] == h[j-1]:\n",
    "\n",
    "                d[i][j] = d[i-1][j-1]\n",
    "\n",
    "            else:\n",
    "\n",
    "                substitute = d[i-1][j-1] + 1\n",
    "\n",
    "                insert = d[i][j-1] + 1\n",
    "\n",
    "                delete = d[i-1][j] + 1\n",
    "\n",
    "                d[i][j] = min(substitute, insert, delete)\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "def getStepList(r, h, d):\n",
    "\n",
    "    '''\n",
    "\n",
    "    This function is to get the list of steps in the process of dynamic programming.\n",
    "\n",
    "    Attributes: \n",
    "\n",
    "        r -> the list of words produced by splitting reference sentence.\n",
    "\n",
    "        h -> the list of words produced by splitting hypothesis sentence.\n",
    "\n",
    "        d -> the matrix built when calulating the editting distance of h and r.\n",
    "\n",
    "    '''\n",
    "\n",
    "    x = len(r)\n",
    "\n",
    "    y = len(h)\n",
    "\n",
    "    list = []\n",
    "\n",
    "    while True:\n",
    "\n",
    "        if x == 0 and y == 0: \n",
    "\n",
    "            break\n",
    "\n",
    "        elif x >= 1 and y >= 1 and d[x][y] == d[x-1][y-1] and r[x-1] == h[y-1]: \n",
    "\n",
    "            list.append(\"e\")\n",
    "\n",
    "            x = x - 1\n",
    "\n",
    "            y = y - 1\n",
    "\n",
    "        elif y >= 1 and d[x][y] == d[x][y-1]+1:\n",
    "\n",
    "            list.append(\"i\")\n",
    "\n",
    "            x = x\n",
    "\n",
    "            y = y - 1\n",
    "\n",
    "        elif x >= 1 and y >= 1 and d[x][y] == d[x-1][y-1]+1:\n",
    "\n",
    "            list.append(\"s\")\n",
    "\n",
    "            x = x - 1\n",
    "\n",
    "            y = y - 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            list.append(\"d\")\n",
    "\n",
    "            x = x - 1\n",
    "\n",
    "            y = y\n",
    "\n",
    "    return list[::-1]\n",
    "\n",
    "\n",
    "\n",
    "def alignedPrint(list, r, h, result):\n",
    "\n",
    "    '''\n",
    "\n",
    "    This funcition is to print the result of comparing reference and hypothesis sentences in an aligned way.\n",
    "\n",
    "    \n",
    "\n",
    "    Attributes:\n",
    "\n",
    "        list   -> the list of steps.\n",
    "\n",
    "        r      -> the list of words produced by splitting reference sentence.\n",
    "\n",
    "        h      -> the list of words produced by splitting hypothesis sentence.\n",
    "\n",
    "        result -> the rate calculated based on edit distance.\n",
    "\n",
    "    '''\n",
    "\n",
    "   # print (\"REF:\",)\n",
    "\n",
    "    for i in range(len(list)):\n",
    "\n",
    "        if list[i] == \"i\":\n",
    "\n",
    "            count = 0\n",
    "\n",
    "            for j in range(i):\n",
    "\n",
    "                if list[j] == \"d\":\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "            index = i - count\n",
    "\n",
    "            #print (\" \"*(len(h[index])),)\n",
    "\n",
    "        elif list[i] == \"s\":\n",
    "\n",
    "            count1 = 0\n",
    "\n",
    "            for j in range(i):\n",
    "\n",
    "                if list[j] == \"i\":\n",
    "\n",
    "                    count1 += 1\n",
    "\n",
    "            index1 = i - count1\n",
    "\n",
    "            count2 = 0\n",
    "\n",
    "            for j in range(i):\n",
    "\n",
    "                if list[j] == \"d\":\n",
    "\n",
    "                    count2 += 1\n",
    "\n",
    "            index2 = i - count2\n",
    "\n",
    "            \"\"\"if len(r[index1]) < len(h[index2]):\n",
    "\n",
    "                print (r[index1] + \" \" * (len(h[index2])-len(r[index1])),)\n",
    "\n",
    "            else:\n",
    "\n",
    "                print (r[index1],)\"\"\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            count = 0\n",
    "\n",
    "            for j in range(i):\n",
    "\n",
    "                if list[j] == \"i\":\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "            index = i - count\n",
    "\n",
    "            #print (r[index],)\n",
    "\n",
    "    #print (\"HYP:\",)\n",
    "\n",
    "    for i in range(len(list)):\n",
    "\n",
    "        if list[i] == \"d\":\n",
    "\n",
    "            count = 0\n",
    "\n",
    "            for j in range(i):\n",
    "\n",
    "                if list[j] == \"i\":\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "            index = i - count\n",
    "\n",
    "            #print (\" \" * (len(r[index])),)\n",
    "\n",
    "        elif list[i] == \"s\":\n",
    "\n",
    "            count1 = 0\n",
    "\n",
    "            for j in range(i):\n",
    "\n",
    "                if list[j] == \"i\":\n",
    "\n",
    "                    count1 += 1\n",
    "\n",
    "            index1 = i - count1\n",
    "\n",
    "            count2 = 0\n",
    "\n",
    "            for j in range(i):\n",
    "\n",
    "                if list[j] == \"d\":\n",
    "\n",
    "                    count2 += 1\n",
    "\n",
    "            index2 = i - count2\n",
    "\n",
    "            \"\"\"if len(r[index1]) > len(h[index2]):\n",
    "\n",
    "                print (h[index2] + \" \" * (len(r[index1])-len(h[index2])),)\n",
    "\n",
    "            else:\n",
    "\n",
    "                print (h[index2],)\"\"\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            count = 0\n",
    "\n",
    "            for j in range(i):\n",
    "\n",
    "                if list[j] == \"d\":\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "            index = i - count\n",
    "\n",
    "            #print (h[index],)\n",
    "\n",
    "    #print (\"EVA:\",)\n",
    "\n",
    "    for i in range(len(list)):\n",
    "\n",
    "        if list[i] == \"d\":\n",
    "\n",
    "            count = 0\n",
    "\n",
    "            for j in range(i):\n",
    "\n",
    "                if list[j] == \"i\":\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "            index = i - count\n",
    "\n",
    "            #print (\"D\" + \" \" * (len(r[index])-1),)\n",
    "\n",
    "        elif list[i] == \"i\":\n",
    "\n",
    "            count = 0\n",
    "\n",
    "            for j in range(i):\n",
    "\n",
    "                if list[j] == \"d\":\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "            index = i - count\n",
    "\n",
    "            #print (\"I\" + \" \" * (len(h[index])-1),)\n",
    "\n",
    "        elif list[i] == \"s\":\n",
    "\n",
    "            count1 = 0\n",
    "\n",
    "            for j in range(i):\n",
    "\n",
    "                if list[j] == \"i\":\n",
    "\n",
    "                    count1 += 1\n",
    "\n",
    "            index1 = i - count1\n",
    "\n",
    "            count2 = 0\n",
    "\n",
    "            for j in range(i):\n",
    "\n",
    "                if list[j] == \"d\":\n",
    "\n",
    "                    count2 += 1\n",
    "\n",
    "            index2 = i - count2\n",
    "\n",
    "            \"\"\"if len(r[index1]) > len(h[index2]):\n",
    "\n",
    "                print (\"S\" + \" \" * (len(r[index1])-1),)\n",
    "\n",
    "            else:\n",
    "\n",
    "                print (\"S\" + \" \" * (len(h[index2])-1),)\"\"\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            count = 0\n",
    "\n",
    "            for j in range(i):\n",
    "\n",
    "                if list[j] == \"i\":\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "            index = i - count\n",
    "\n",
    "            #print (\" \" * (len(r[index])))\n",
    "\n",
    "\n",
    "\n",
    "    #print (\"WER: \" + result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def wer(r, h):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    This is a function that calculate the word error rate in ASR.\n",
    "\n",
    "    You can use it like this: wer(\"what is it\".split(), \"what is\".split()) \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # build the matrix\n",
    "\n",
    "    d = editDistance(r, h)\n",
    "\n",
    "\n",
    "\n",
    "    # find out the manipulation steps\n",
    "\n",
    "    list = getStepList(r, h, d)\n",
    "\n",
    "\n",
    "\n",
    "    # print the result in aligned way\n",
    "\n",
    "    #result = float(d[len(r)][len(h)]) / len(r) * 100\n",
    "\n",
    "    result = float(d[len(r)][len(h)]) / len(r)\n",
    "\n",
    "    #result = str(\"%.2f\" % result) + \"%\"\n",
    "\n",
    "    WER = alignedPrint(list, r, h, result)\n",
    "\n",
    "    return WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It turned out that some sentences have whitespace in the beginning. that is why I decided to implement this function\n",
    "def delete_first_whitespace(line):\n",
    "    ind = 0 \n",
    "    while (line[ind] == ' '):\n",
    "        ind += 1\n",
    "    return line [ind:]\n",
    "    \n",
    "delete_first_whitespace('   test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_parsed = \"C://github//ftyers.github.io//2018-komp-ling//practicals//Tokenisation//sentence_test_parsed.txt\"\n",
    "coorect_parsed = \"C://github//ftyers.github.io//2018-komp-ling//practicals//Tokenisation//sentences_test_correctly_parsed.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23076923076923078\n"
     ]
    }
   ],
   "source": [
    "with open(coorect_parsed, \"r\", encoding = 'utf-8') as reference, open(sentences_parsed, \"r\", encoding = 'utf-8') as hypo: \n",
    "    wer_overall = []\n",
    "    print_count = 0 \n",
    "    for r, h in zip(reference, hypo):\n",
    "        #get rid of whitespaces in front\n",
    "        r = delete_first_whitespace(r)\n",
    "        h = delete_first_whitespace(h)\n",
    "        #get list of tokenized words\n",
    "        r = r.split()\n",
    "        h = h.split()\n",
    "        wer_ind = wer(r,h)\n",
    "        wer_overall.append(wer_ind)\n",
    "\n",
    "    overall_wer_median = statistics.median(wer_overall)\n",
    "    print(overall_wer_median)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
