{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поледовательность задач:\n",
    "1. Создадим список уникальных словоформ (wordlist) из тренировочного набора UD_Japanese-GSD.\n",
    "2. Получим из тестового набора UD_Japanese-GSD список исходных предложений для токенизации (sentences).\n",
    "3. Получим из тестового набора UD_Japanese-GSD список предложений, уже разбитых на токены (reference_sentences).\n",
    "4. Используем max_match для токенизации тестового набора исходных предложений (sentences).\n",
    "5. Посчитаем качество токенизации (WER). Вычислим расстояние Левенштейна для списка токенов исходного предложения (reference_sentence) и списка токенов, полученных при токенизации предложения алгоритмом maxmatch (hypothesis_sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция, которая создает список уникальных словоформ (wordlist) для использвоания в max_match\n",
    "def wordlist_extract(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        l = f.readlines()\n",
    "        wordlist = []\n",
    "        for line in l:\n",
    "            if line[0].isdigit():\n",
    "                s = line.split(\"\\t\")\n",
    "                wordlist.append(s[1])\n",
    "        wordlist = set(wordlist)\n",
    "        return list(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Создадим список уникальных словоформ из тренировочного набора UD_Japanese-GSD\n",
    "wordlist = wordlist_extract('ja_gsd-ud-train.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22313"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# всего получилось 22 313 словоформ\n",
    "len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['藩', 'フース', '認める', '往々', '中間報告', 'ビール', '馬小屋', 'アイディア', '懐疑的', '敷き詰める']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим первые десть\n",
    "wordlist[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-3. Вытащим предложения для токенизации и токены для референса из тестового набора UD_Japanese-GSD\n",
    "sentences = []\n",
    "reference_sentences = []\n",
    "print_count = 0\n",
    "with open('ja_gsd-ud-test.conllu', \"r\", encoding = 'utf-8') as data:\n",
    "    parsed_sentence = [] \n",
    "    for line in data:\n",
    "        row = line.split('\\t')\n",
    "        if('# text =' in row[0]):\n",
    "            sentences.append(row[0][8:-1])\n",
    "        if(str(row[0]).isdigit()):\n",
    "            parsed_sentence.append(row[1])\n",
    "        else:\n",
    "            if parsed_sentence:\n",
    "                reference_sentences.append(parsed_sentence)\n",
    "            parsed_sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' これに不快感を示す住民はいましたが,現在,表立って反対や抗議の声を挙げている住民はいないようです。',\n",
       " ' 幸福の科学側からは,特にどうしてほしいという要望はいただいていません。']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на два исходных предложения\n",
    "sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['これ',\n",
       "  'に',\n",
       "  '不快感',\n",
       "  'を',\n",
       "  '示す',\n",
       "  '住民',\n",
       "  'は',\n",
       "  'い',\n",
       "  'まし',\n",
       "  'た',\n",
       "  'が',\n",
       "  ',',\n",
       "  '現在',\n",
       "  ',',\n",
       "  '表立っ',\n",
       "  'て',\n",
       "  '反対',\n",
       "  'や',\n",
       "  '抗議',\n",
       "  'の',\n",
       "  '声',\n",
       "  'を',\n",
       "  '挙げ',\n",
       "  'て',\n",
       "  'いる',\n",
       "  '住民',\n",
       "  'は',\n",
       "  'い',\n",
       "  'ない',\n",
       "  'よう',\n",
       "  'です',\n",
       "  '。'],\n",
       " ['幸福',\n",
       "  'の',\n",
       "  '科学',\n",
       "  '側',\n",
       "  'から',\n",
       "  'は',\n",
       "  ',',\n",
       "  '特に',\n",
       "  'どうして',\n",
       "  'ほしい',\n",
       "  'という',\n",
       "  '要望',\n",
       "  'は',\n",
       "  'いただい',\n",
       "  'て',\n",
       "  'い',\n",
       "  'ませ',\n",
       "  'ん',\n",
       "  '。']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на токены, которые получились из исходных предложений\n",
    "reference_sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_match\n",
    "# sentence – предложение, которое предстоит токенизировать\n",
    "# wordlist - список уникальных словоформ, по которым происходит поиск токенов\n",
    "def max_match(sentence, wordlist):\n",
    "    if not sentence:\n",
    "        return []\n",
    "    for i in range(len(sentence)-1, -1, -1):\n",
    "        first_word = (sentence[0:i+1])\n",
    "        remainder = sentence[i+1:len(sentence)]\n",
    "        if first_word in wordlist:\n",
    "            return [first_word] + max_match(remainder, wordlist)\n",
    "\n",
    "    # если слово не найдено, то создаем новое односимвольное слово\n",
    "    first_word = sentence[0]\n",
    "    remainder = sentence[1:len(sentence)]\n",
    "\n",
    "    return [first_word] + max_match(remainder, wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Токинизируем max_match'ем первые 100 предложений\n",
    "hypothesis_sentences = []\n",
    "for sentence in sentences[0:99]:\n",
    "    hypothesis_sentence = max_match(sentence.strip(), wordlist)\n",
    "    hypothesis_sentences.append(hypothesis_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['これ',\n",
       "  'に',\n",
       "  '不快',\n",
       "  '感',\n",
       "  'を',\n",
       "  '示す',\n",
       "  '住民',\n",
       "  'は',\n",
       "  'いま',\n",
       "  'し',\n",
       "  'たが',\n",
       "  ',',\n",
       "  '現在',\n",
       "  ',',\n",
       "  '表',\n",
       "  '立っ',\n",
       "  'て',\n",
       "  '反対',\n",
       "  'や',\n",
       "  '抗議',\n",
       "  'の',\n",
       "  '声',\n",
       "  'を',\n",
       "  '挙げて',\n",
       "  'いる',\n",
       "  '住民',\n",
       "  'は',\n",
       "  'い',\n",
       "  'ない',\n",
       "  'ようで',\n",
       "  'す',\n",
       "  '。'],\n",
       " ['幸福',\n",
       "  'の',\n",
       "  '科学',\n",
       "  '側',\n",
       "  'から',\n",
       "  'は',\n",
       "  ',',\n",
       "  '特に',\n",
       "  'どうして',\n",
       "  'ほしい',\n",
       "  'という',\n",
       "  '要望',\n",
       "  'は',\n",
       "  'いただい',\n",
       "  'て',\n",
       "  'いま',\n",
       "  'せ',\n",
       "  'ん',\n",
       "  '。']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на два первых предложения, токенизированных max_match'ем\n",
    "hypothesis_sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для расчета качества токенизации (WER)\n",
    "# https://martin-thoma.com/word-error-rate-calculation/\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "\n",
    "def editDistance(r, h):\n",
    "    '''\n",
    "    This function is to calculate the edit distance of reference sentence and the hypothesis sentence.\n",
    "    Main algorithm used is dynamic programming.\n",
    "    Attributes: \n",
    "        r -> the list of words produced by splitting reference sentence.\n",
    "        h -> the list of words produced by splitting hypothesis sentence.\n",
    "    '''\n",
    "    d = numpy.zeros((len(r)+1)*(len(h)+1), dtype=numpy.uint8).reshape((len(r)+1, len(h)+1))\n",
    "    for i in range(len(r)+1):\n",
    "        for j in range(len(h)+1):\n",
    "            if i == 0: \n",
    "                d[0][j] = j\n",
    "            elif j == 0: \n",
    "                d[i][0] = i\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                d[i][j] = d[i-1][j-1]\n",
    "            else:\n",
    "                substitute = d[i-1][j-1] + 1\n",
    "                insert = d[i][j-1] + 1\n",
    "                delete = d[i-1][j] + 1\n",
    "                d[i][j] = min(substitute, insert, delete)\n",
    "    return d\n",
    "\n",
    "\n",
    "def getStepList(r, h, d):\n",
    "    '''\n",
    "    This function is to get the list of steps in the process of dynamic programming.\n",
    "    Attributes: \n",
    "        r -> the list of words produced by splitting reference sentence.\n",
    "        h -> the list of words produced by splitting hypothesis sentence.\n",
    "        d -> the matrix built when calulating the editting distance of h and r.\n",
    "    '''\n",
    "    x = len(r)\n",
    "    y = len(h)\n",
    "    list = []\n",
    "    while True:\n",
    "        if x == 0 and y == 0: \n",
    "            break\n",
    "        elif x >= 1 and y >= 1 and d[x][y] == d[x-1][y-1] and r[x-1] == h[y-1]: \n",
    "            list.append(\"e\")\n",
    "            x = x - 1\n",
    "            y = y - 1\n",
    "        elif y >= 1 and d[x][y] == d[x][y-1]+1:\n",
    "            list.append(\"i\")\n",
    "            x = x\n",
    "            y = y - 1\n",
    "        elif x >= 1 and y >= 1 and d[x][y] == d[x-1][y-1]+1:\n",
    "            list.append(\"s\")\n",
    "            x = x - 1\n",
    "            y = y - 1\n",
    "        else:\n",
    "            list.append(\"d\")\n",
    "            x = x - 1\n",
    "            y = y\n",
    "    return list[::-1]\n",
    "\n",
    "\n",
    "def wer(r, h):\n",
    "    \"\"\"\n",
    "    This is a function that calculate the word error rate in ASR.\n",
    "    You can use it like this: wer(\"what is it\".split(), \"what is\".split()) \n",
    "    \"\"\"\n",
    "    # build the matrix\n",
    "    d = editDistance(r, h)\n",
    "\n",
    "    # find out the manipulation steps\n",
    "    list = getStepList(r, h, d)\n",
    "\n",
    "    # print the result in aligned way\n",
    "    result = float(d[len(r)][len(h)]) / len(r) * 100\n",
    "    #result = str(\"%.2f\" % result) + \"%\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Посчиатем ошибку для первого пердложения\n",
    "wer(reference_sentences[0], hypothesis_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посчиатем ошибку для первых ста пердложений\n",
    "wer_for_sentences = []\n",
    "for i in range(99):\n",
    "    wer_for_sentence = wer(reference_sentences[i], hypothesis_sentences[i])\n",
    "    wer_for_sentences.append(wer_for_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.858267979839734\n"
     ]
    }
   ],
   "source": [
    "# Медианное значение ошибки, рассчитанной для первых ста предложений\n",
    "print(numpy.mean(wer_for_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
