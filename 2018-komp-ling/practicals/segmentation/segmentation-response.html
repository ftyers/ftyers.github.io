<html>
<body>

<h2>Segmentation</h2>

<p>Comparison of Pragmatic Segmenter and NLTK's Punkt.</p>

<h3>Data</h3>

<p>Extracting the text: <xmp>$ python3 WikiExctractor.py --infn data.bz2 >wiki.txt</xmp></p>
<p>Shuffle: <xmp>$ sort -R <wiki.txt >randomwiki.txt</xmp></p>
<p>Get 50 paragraphs: <xmp>$ head -n 50 <randomwiki.txt >random50.txt</xmp></p>

<h3>Pragmatic Segmenter</h3>

<p>A rule-based Ruby package for sentence segmentation, works out-of-the-box across many languages.</p>
<p>Segmentation of wiki paragraphs using pragmatic segmenter:<br> 
	<xmp>$ ruby -I . segmenter.rb <random50.txt >pragmsegm_results.txt</xmp></p>
<p>Results are in <i>'pragmsegm_results.txt'</i> file.</p>

<h3>NLTK's Punkt</h3>

<p>Machine learning sentence and word tokenizer for Python.</p>
<p>I used pre-trained model for Russian from <a href='https://github.com/Mottl/ru_punkt'>https://github.com/Mottl/ru_punkt</a>.</p>
<p>Python code for tokenizing sentences from stdin is in <i>'sent_tokenize.py'</i> file, the results for Punkt<br>
	are in <i>'nltk_results.txt'</i>.</p>
<p>Bash command: <xmp>$ python sent_tokenize.py <random50.txt >nltk_results.txt</xmp></p>

<h3>Evaluation</h3>

<h2>Tokenization</h2>

</body>
</html>
