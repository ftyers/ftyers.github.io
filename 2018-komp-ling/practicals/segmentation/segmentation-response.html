<html>
<body>

<h2>Segmentation</h2>

<p>Comparison of Pragmatic Segmenter and NLTK's Punkt.</p>

<h3>Data</h3>

<p>Extracting the text: $ python3 WikiExctractor.py --infn data.bz2 >wiki.txt</p>
<p>Shuffle: $ sort -R <wiki.txt >randomwiki.txt</p>
<p>Get 50 paragraphs: $ head -n 50 <randomwiki.txt >random50.txt</p>

<h3>Pragmatic Segmenter</h3>

<p>A rule-based Ruby package for sentence segmentation, works out-of-the-box across many languages.</p>
<p>Segmentation of wiki paragraphs using pragmatic segmenter: $ ruby -I . segmenter.rb <random50.txt >pragmsegm_results.txt.</p>
<p>Results are in 'pragmsegm_results.txt' file.</p>

<h3>NLTK's Punkt</h3>

<p>Machine learning sentence and word tokenizer for Python.</p>

<h3>Evaluation</h3>

<h2>Tokenization</h2>

</body>
</html>
