## Отчет по сегментации

Для сегментации предложений я использовал Punkt Sentence Tokenizer из библиотеки NLTK и модуль из библиотеку spaCy.

Punkt Sentence Tokenizer делит текст на предложения, используя алгоритм машинного обучение без учителя, который строит модель сокращений, коллокаций и слов, с которых начинаются предложения. Перед применением его необходимо предварительно обучить на большой коллекции документов соответствующего языка.

В spaCy для определения границ предложения используется анализ на основе деревьев зависимостей (dependency parsing). Такой подход зачастую дает более точный результат, чем подход основанный на правилах, но требует статистической модели. Для текстов новостей и статей он неплохо работает даже в базовой реализации. Для текстов из соцсетей или разговоров, где сложно выделить паттерны, может заметно превзойти решение, построенное на правилах. Позволяет подключить к пайплайну обработки текста компонент на основе правил с пользовательскими настройками (SentenceSegmenter).


### Качество сегментации

**Punkt Sentence Tokenizer** допустил четыре ошибки. Не справился с несколькими аббревиатурами: i.e., et al., e.g. С другой стороны с инициалами, прямой речью и оставшейся частью аббревиатур все корректно. Пример ошибки:

> The visit of the three pilgrim "Scots" (i.e.

> Irish) to Alfred in 891 is undoubtedly authentic.

**SpaCy** допустил две одинаковые ошибки. Не справился с et al., также как и Punkt Sentence Tokenizer. Пример:

> The excitation methodology is described by Itano et al.

> and the time needed for it is given by the Rabi flopping formula.

[Подробный код исследования](Segmentation.md)

