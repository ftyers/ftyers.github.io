<!--
SPDX-License-Identifier: (CC-BY-SA-4.0 OR GFDL-1.3-or-later)
Copyright 2018 Nick Howell
-->

<div style="column-width: 30em">

<h1> Обзор двух библиотек по токенизированию предложений  </h1>

В данном отчете было использованы две библиотеки по токенеизорванию спредложений из текста: pragmatic segmenter (Ruby) и NLTK (Python). В качестве тестового текста был использован кусок дампа русской Википедии.
<h2> Pragmatic segmenter (Ruby) </h2>
Pragmatic segmenter - это бибилотека для Ruby, основанная на правилах. При парсинге русской википедии данная библиотека показала качество ниже среднего. Большинство сокращений, инициалы имен и т.д. неправильно были разделены на предложения.
В общем библиотека больше расчитана на языки латинского алфавита.

<h2> NLTK (Python) </h2>

sent_tokenize() - это функция библиотеки NLTK по определению границ предложения. Но на самом деле это алгоритм машинного обучения без учителя, который можно обучить самомстоятельно. В бибилотеке NLTK уже есть набор pre-trained моделей, в том числе и для русского языка. В общем данная библиотека показала себя лучше, чем Ruby. Большинство сокращений и инициалов выделены правильно, единтсвенную проблему составляет сокращения с пробелами внутри.

</div>
